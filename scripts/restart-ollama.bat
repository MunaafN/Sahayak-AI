@echo off
echo ================================================
echo ğŸ”„ RESTARTING OLLAMA SERVICE (Memory Fix)
echo ================================================

echo.
echo ğŸ›‘ Stopping Ollama service...
taskkill /f /im ollama.exe 2>nul
timeout /t 2 /nobreak >nul

echo âœ… Ollama stopped
echo.
echo ğŸ§  Clearing memory...
timeout /t 3 /nobreak >nul

echo.
echo ğŸš€ Starting Ollama service...
start /min ollama serve

echo â³ Waiting for service to start...
timeout /t 5 /nobreak >nul

echo.
echo ğŸ” Testing service availability...
curl -s http://localhost:11434/api/tags >nul
if %errorlevel% equ 0 (
    echo âœ… Ollama service is running
) else (
    echo âŒ Ollama service failed to start
    echo ğŸ’¡ Please install Ollama from https://ollama.ai/
    pause
    exit /b 1
)

echo.
echo ğŸ“¥ Ensuring models are available...
ollama pull llama3:8b
ollama pull llava-phi3:latest

echo.
echo ================================================
echo âœ… OLLAMA RESTART COMPLETE
echo ================================================
echo ğŸ§  Memory-efficient models ready:
echo    - llama3:8b (text generation)
echo    - llava-phi3:latest (vision)
echo.
echo ğŸ’¡ You can now start the backend with: python main.py
echo ================================================

pause 